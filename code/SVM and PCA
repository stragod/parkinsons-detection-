
#import required libraries
import pandas as pd 
import numpy as np
import warnings
import matplotlib.pyplot as plt 
from matplotlib.pyplot import figure
import seaborn as sns 
from scipy import stats
from pandas import DataFrame 
from sklearn.decomposition import PCA
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn import metrics 
from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler
from sklearn import svm
from sklearn.model_selection import train_test_split 
from sklearn.metrics import precision_recall_curve
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
warnings.filterwarnings("ignore") 
from sklearn.preprocessing import MinMaxScaler
import sklearn.model_selection as model_selection
from sklearn.metrics import classification_report,precision_score,recall_score,f1_score,hamming_loss,matthews_corrcoef,accuracy_score,roc_auc_score,plot_confusion_matrix



#read the dataset
df = pd.read_csv(r"C:\Users\ragha\OneDrive - SRH IT\Documents\parkinson detection\parkinsons.data",header=0)


#printing the first 5 rows of the data 
df.head()


# Description of data 
df.describe()

# understand data types of the column data 
df.info()

#understand number of rows and columns in the dataset 
print(df.shape)

#check for null values in the dataset
print(df.isnull().sum())

print(df.describe())

# distribution of features 
df.hist(figsize=(20,20),bins=15)
plt.title("Features Distribution")
plt.show()

#Heatmap correlation
plt.figure(figsize =(20,10))
sns.heatmap(df.corr(),cmap= "Greens",square= True,linewidth = 0.2)
plt.title("Feature Correlation matrix")
plt.show()


# boxplot to see distribution of data and determine the number of outliers 
plt.figure(figsize=(13,10))
sns.boxplot(data=df,palette = "Set1")
plt.xticks(rotation=90)
plt.show()

#number of cases with parkinsons and without parkinsons in the dataset
sns.countplot(x="status",data= df)
plt.show()

df.status.value_counts()

x = df.drop(columns=['status','name'],axis=1)
y = df['status']
X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.2, random_state=2)
scaler = StandardScaler()
scaler.fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)


# Training and testing data using SVM
model = svm.SVC(kernel='linear')
model.fit(X_train, Y_train)
X_train_pred = model.predict(X_train)
training_data_accuracy = accuracy_score(Y_train, X_train_pred)
print('Accuracy of Training Data :', training_data_accuracy*100, '%')
X_test_pred = model.predict(X_test)
testing_data_accuracy = accuracy_score(Y_test, X_test_pred)
print('Accuracy of Testing Data :', testing_data_accuracy*100, '%')

# data scaling

data_rescaled = MinMaxScaler().fit_transform(x)
data_rescaled

# #PCA analysis for 2 principal components
pca = PCA(n_components = 2)
pca.fit(data_rescaled)
reduced = pca.transform(data_rescaled)
#  make them into another dataframe and add the status column to the modified dataframe 
df_reduced = pd.DataFrame(reduced,columns=['PC1','PC2'])
df_reduced['status'] = df['status']
df_reduced


#plot the PCA against each other to see distribution of data
plt.figure(num=None, figsize=(8, 8), dpi=80, facecolor='w', edgecolor='k')

classes = [1, 0]
colors = ['r', 'b']
for clas, color in zip(classes, colors):
    plt.scatter(df_reduced.loc[df_reduced['status'] == clas, 'PC1'], 
                df_reduced.loc[df_reduced['status'] == clas, 'PC2'], 
                c = color)
    
plt.xlabel('Principal Component 1', fontsize = 12)
plt.ylabel('Principal Component 2', fontsize = 12)
plt.title('2D PCA', fontsize = 15)
plt.legend(['Healthy', 'PD'])
plt.grid()

#PCA analysis for 3 principal components
pca = PCA(n_components = 3)
pca.fit(data_rescaled)
reduced2 = pca.transform(data_rescaled)
# make them into another dataframe and add the status column to the modified dataframe 
df_reduced2 = pd.DataFrame(reduced2,columns=['PC1','PC2','PC3'])
df_reduced2['status'] = df['status']
df_reduced2

#3d plot to get view of data distribution 
fig = plt.figure()
ax = plt.axes(projection='3d')
classes = [1, 0]
colors = ['r', 'b']
#print(df_reduced2.loc[df_reduced['status'] == clas, 'PC1'])
# for clas ,color in (classes,colors):
#df_reduced2.iloc[df_reduced['status']==clas]
for clas, color in zip(classes, colors):
    ax.scatter3D(df_reduced2.loc[df_reduced['status'] == clas, 'PC1'], 
                df_reduced2.loc[df_reduced['status'] == clas, 'PC2'],
                df_reduced2.loc[df_reduced['status'] == clas, 'PC3'],
                c = color,alpha=1) # alpha = 1 added for removing transparency which is default setting added to 3d plot
ax.set_xlabel('Principal Component 1', fontsize = 12)
ax.set_ylabel('Principal Component 2', fontsize = 12)
ax.set_zlabel('Principal Component 3', fontsize =12)
ax.set_title('3D PCA graph ', fontsize = 15)
ax.legend(['Healthy', 'PD'])



#splitting dataset into training and testing  
x = df_reduced2.drop('status',axis=1)
y = df['status']
x_train, x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)


#SVM for PCA components (non-optimized)
model1 = svm.SVC(kernel='linear')
model1.fit(x_train, y_train)
x_train_pred = model1.predict(x_train)
training_data_accuracy = accuracy_score(y_train, x_train_pred)
print('Accuracy of Training Data :', training_data_accuracy*100, '%')
x_test_pred = model1.predict(x_test)
testing_data_accuracy = accuracy_score(y_test, x_test_pred)
print('Accuracy of Testing Data :', testing_data_accuracy*100, '%')



